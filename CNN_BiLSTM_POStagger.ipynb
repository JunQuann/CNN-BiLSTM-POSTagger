{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-BiLSTM-POStagger.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunQuann/CNN-BiLSTM-POSTagger/blob/master/CNN_BiLSTM_POStagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1lZ6WMHS2rn",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ6JDejXr3qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS__ZpxXr8GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/gdrive/My Drive/released'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aVNhYit1S2rs",
        "colab": {}
      },
      "source": [
        "char2Idx = {\"PAD\":0, \"UNK\":1}\n",
        "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
        "    char2Idx[c] = len(char2Idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EaKFzmzUS2ru",
        "colab": {}
      },
      "source": [
        "CHAR_EMBEDDING_DIM = 30\n",
        "char_embedding_weights = torch.FloatTensor(len(char2Idx), CHAR_EMBEDDING_DIM).uniform_(-math.sqrt(3/CHAR_EMBEDDING_DIM), math.sqrt(3/CHAR_EMBEDDING_DIM))\n",
        "char_embedding_weights[0] = torch.zeros(CHAR_EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "exk4pQjSS2rw",
        "colab": {}
      },
      "source": [
        "WORD_EMBEDDING_DIM = 100 #glove.6b.50d\n",
        "word2Idx_pkl_path = 'w2i.6B.100d.pkl'\n",
        "weembed_pkl_path = 'wembed.6B.100d.pkl'\n",
        "\n",
        "if os.path.exists(word2Idx_pkl_path) and os.path.exists(weembed_pkl_path):\n",
        "    word2Idx = pickle.load(open(word2Idx_pkl_path, 'rb'))\n",
        "    word_embedding_weights = pickle.load(open(weembed_pkl_path, 'rb'))\n",
        "\n",
        "else:\n",
        "    word2Idx = {}\n",
        "    word_embedding_weights = []\n",
        "    with open(\"glove.6B.100d.txt\", 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            \n",
        "            if len(word2Idx) == 0: #Add unknown\n",
        "                word2Idx[\"UNK\"] = len(word2Idx)\n",
        "                vector = torch.FloatTensor(1, WORD_EMBEDDING_DIM).uniform_(-math.sqrt(3/WORD_EMBEDDING_DIM), math.sqrt(3/WORD_EMBEDDING_DIM))\n",
        "                word_embedding_weights.append(vector)\n",
        "\n",
        "            vector = [float(num) for num in values[1:]]\n",
        "            word_embedding_weights.append(torch.tensor(vector, dtype=torch.float32).reshape(1,-1))\n",
        "            word2Idx[word] = len(word2Idx)\n",
        "            \n",
        "    word_embedding_weights = torch.cat(word_embedding_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH7ozgvS2nEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(word2Idx, open('w2i.6B.100d.pkl', 'wb'))\n",
        "pickle.dump(word_embedding_weights, open('wembed.6B.100d.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mco-3GK6S2r2",
        "colab": {}
      },
      "source": [
        "class CNN_BiLSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                 char_embedding_weights, \n",
        "                 word_embedding_weights, \n",
        "                 tagset_size, \n",
        "                 num_filters=30,\n",
        "                 kernel_size=3,\n",
        "                 hidden_dim=200,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.5\n",
        "                ):\n",
        "        super(CNN_BiLSTM, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(word_embedding_weights, freeze=False)\n",
        "        self.char_embedding = nn.Embedding.from_pretrained(char_embedding_weights, freeze=False)\n",
        "        \n",
        "        self.conv_layer = nn.Sequential(\n",
        "                        nn.Dropout(p=dropout),\n",
        "                        nn.Conv1d(self.char_embedding.embedding_dim, num_filters, kernel_size),\n",
        "                        nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        \n",
        "        self.dropout_char_rep = nn.Dropout(p=dropout)\n",
        "        \n",
        "        self.bi_lstm = nn.LSTM(self.word_embedding.embedding_dim + num_filters, hidden_dim, \n",
        "                               num_layers=num_layers, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * num_layers * 2, tagset_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, sentence, words):\n",
        "        embeds = self.word_embedding(sentence)\n",
        "        char_hidden_final = []\n",
        "        \n",
        "        for word in words:\n",
        "            pad = torch.zeros(self.kernel_size // 2, dtype=torch.long, device=self.device)\n",
        "            padded_word = torch.cat((pad, word, pad))\n",
        "            char_embeds = self.char_embedding(padded_word)\n",
        "            char_representation = self.conv_layer(char_embeds.view(1, -1, len(char_embeds)))\n",
        "            char_hidden_final.append(char_representation.view(-1))\n",
        "            \n",
        "        char_hidden_final = torch.stack(tuple(char_hidden_final))\n",
        "        \n",
        "        combined = torch.cat((embeds, char_hidden_final), 1)\n",
        "        \n",
        "        dropout_combined = self.dropout_char_rep(combined)\n",
        "        lstm_out, _ = self.bi_lstm(dropout_combined.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vn2DoHfeS2r4",
        "colab": {}
      },
      "source": [
        "def str2tuple(s, sep='/'):\n",
        "    loc = s.rfind(sep)\n",
        "    if loc >= 0:\n",
        "        return (s[:loc], s[loc + len(sep) :].upper())\n",
        "    else:\n",
        "        return (s, None) \n",
        "    \n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = []\n",
        "    for w in (seq):\n",
        "        try:\n",
        "            idxs.append(to_ix[w])\n",
        "        except KeyError:\n",
        "            idxs.append(to_ix['UNK'])\n",
        "        \n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TlfsSgnvS2r6",
        "colab": {}
      },
      "source": [
        "tag2idx = {}\n",
        "tagged_sentences = []\n",
        "\n",
        "    \n",
        "with open('sents.train', 'r') as f:\n",
        "    for line in f:\n",
        "        tagged_words = [str2tuple(word) for word in line.split()]\n",
        "        tagged_sentences.append(tagged_words)\n",
        "        for tagged_word in tagged_words:\n",
        "\n",
        "            tag = tagged_word[1]\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "\n",
        "word_embedding_weights = torch.FloatTensor(len(word2Idx), WORD_EMBEDDING_DIM).uniform_(-math.sqrt(3/WORD_EMBEDDING_DIM), math.sqrt(3/WORD_EMBEDDING_DIM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cQ8y3XzLS2r8",
        "colab": {}
      },
      "source": [
        "NUM_FILTER = 30\n",
        "KERNEL_SIZE = 3\n",
        "HIDDEN_DIM = 200\n",
        "NUM_LAYERS = 1\n",
        "DROPOUT = 0.5\n",
        "EPOCH = 5\n",
        "SUBSAMPLE_SIZE = 1\n",
        "LEARNING_RATE = 0.01\n",
        "DECAY_RATE = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zf1oEjdUS2r-",
        "colab": {}
      },
      "source": [
        "model = CNN_BiLSTM(char_embedding_weights, word_embedding_weights, len(tag2idx), num_filters=NUM_FILTER, \n",
        "                   kernel_size=KERNEL_SIZE, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
        "\n",
        "saved_model_path = 'CNN_BiLSTM.pt'\n",
        "if os.path.exists(saved_model_path):\n",
        "  model.load_state_dict(torch.load(saved_model_path))\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "# The test sentence\n",
        "check = 'Just a random sentence .'.split()\n",
        "print(\"Running a check on the model before training.\\nSentences:\\n{}\".format(\" \".join(check)))\n",
        "with torch.no_grad():\n",
        "    words = [prepare_sequence(s, char2Idx).to(device) for s in check]\n",
        "    sentence = prepare_sequence(map(lambda word: word.lower(), check), word2Idx).to(device)\n",
        "        \n",
        "    tag_scores = model(sentence, words)\n",
        "    _, indices = torch.max(tag_scores, 1)\n",
        "    ret = []\n",
        "    for i in range(len(indices)):\n",
        "        for key, value in tag2idx.items():\n",
        "            if indices[i] == value:\n",
        "                ret.append((check[i], key))\n",
        "    print(ret)\n",
        "\n",
        "lambda1 = lambda epoch: 1 / (1 + DECAY_RATE * epoch)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "model = model.train()\n",
        "start = time.time()\n",
        "for epoch in range(EPOCH):\n",
        "    subsample_tagged_sentences = random.sample(tagged_sentences, int(SUBSAMPLE_SIZE * len(tagged_sentences)))\n",
        "    \n",
        "    for i, tagged_sentence in enumerate(subsample_tagged_sentences):\n",
        "        \n",
        "        seq = [tagged_word[0] for tagged_word in tagged_sentence]\n",
        "        tags = [tagged_word[1] for tagged_word in tagged_sentence]\n",
        "        words = [prepare_sequence(s, char2Idx).to(device) for s in seq]\n",
        "        sentence = prepare_sequence(map(lambda word: word.lower(), seq), word2Idx).to(device)\n",
        "        targets = prepare_sequence(tags, tag2idx).to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        tag_scores = model(sentence, words)\n",
        "    \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    scheduler.step()\n",
        "    epoch_end_time = time.time()\n",
        "    print('Epoch %d completed in %d minutes %d seconds' % (epoch + 1, (epoch_end_time - start) // 60, (epoch_end_time - start) % 60))\n",
        "\n",
        "        \n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    words = [prepare_sequence(s, char2Idx).to(device) for s in check]\n",
        "    sentence = prepare_sequence(map(lambda word: word.lower(), check), word2Idx).to(device)\n",
        "        \n",
        "    tag_scores = model(sentence, words)\n",
        "    _, indices = torch.max(tag_scores, 1)\n",
        "    ret = []\n",
        "    for i in range(len(indices)):\n",
        "        for key, value in tag2idx.items():\n",
        "            if indices[i] == value:\n",
        "                ret.append((check[i], key))\n",
        "    print(ret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUp8F8criFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'CNN_BiLSTM.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGoUBOyqS2sD",
        "colab": {}
      },
      "source": [
        "test_sentences = []\n",
        "\n",
        "with open('sents.test', 'r') as f:\n",
        "    for line in f:\n",
        "        test_seq = line.split()\n",
        "        test_sentences.append(test_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PmwlqIrbS2sF",
        "colab": {}
      },
      "source": [
        "test_tagged = []\n",
        "\n",
        "model = model.eval()\n",
        "for test_sentence in test_sentences:\n",
        "    words = [prepare_sequence(s, char2Idx).to(device) for s in test_sentence]\n",
        "    sentence = prepare_sequence(map(lambda word: word.lower(), test_sentence), word2Idx).to(device)\n",
        "\n",
        "    tag_scores = model(sentence, words)\n",
        "    _, indices = torch.max(tag_scores, 1)\n",
        "    ret = []\n",
        "    for i in range(len(indices)):\n",
        "        for key, value in tag2idx.items():\n",
        "            if indices[i] == value:\n",
        "                ret.append((test_sentence[i], key))\n",
        "    test_tagged.append(ret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9V4XtMJ1S2sH",
        "colab": {}
      },
      "source": [
        "with open('sents.out', 'w') as f:\n",
        "    for sentence in test_tagged:\n",
        "        formatted_sentence = ' '.join(map(lambda t: '%s%s%s' % (t[0], '/', t[1]), sentence))\n",
        "        f.write(formatted_sentence + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "epKe5L5JS2sJ",
        "colab": {}
      },
      "source": [
        "%run eval.py sents.out sents.answer"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}